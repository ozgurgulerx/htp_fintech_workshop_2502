{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building with AutoGen \n",
    "### Please Complete the **[environment setup](part1_environment_setup.ipynb)** first before progressing with the notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Starting Building with AutoGen \n",
    "Let's start with a simple example of how AutoGen is integrated Azure OpenAI. \n",
    "The example will confirm Azure OpenAI endpoints and keys are correctly configured and that AutoGen is installed and working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Create a minimal Azure OpenAI client.\n",
    "azure_client = AzureOpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Create an assistant agent with a simple system message.\n",
    "agent = AssistantAgent(\n",
    "    name=\"azure_test_agent\",\n",
    "    model_client=azure_client,\n",
    "    system_message=\"You are a helpful Azure test assistant.\"\n",
    ")\n",
    "\n",
    "async def test_connection():\n",
    "    user_input = \"Hello! Are you working?\"\n",
    "    await Console(agent.run_stream(task=user_input))\n",
    "\n",
    "await test_connection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **autogen** framework provides multiple agent rolesâ€”such as `AssistantAgent`, `UserAgent`, `FunctionAgent` (for function or tool calls), and `MultiAgent` (for orchestrating multiple participants)â€”each designed for specialized behaviors in a conversation. For example:\n",
    "\n",
    "- `AssistantAgent`: Represents a helpful AI assistant.\n",
    "- `FunctionAgent`: Handles specific tools or API calls.\n",
    "- `MultiAgent`: Coordinates interactions among different agents.\n",
    "\n",
    "This modular design makes it straightforward to build and manage multi-turn, multi-role dialogues, assigning clear, distinct responsibilities to each agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AssistantAgent` we used here is a high-level wrapper around a language model client (such as an AzureOpenAI model) that manages:\n",
    "\n",
    "1. **Conversational Context**: It keeps track of all previous exchanges and organizes them into a coherent dialog.\n",
    "2. **System or Instruction Prompt**: You can give it a â€œsystem messageâ€ that defines the overarching instructions or persona, such as:\n",
    "   > \"You are a helpful Azure test assistant.\"\n",
    "\n",
    "By consolidating these elements, `AssistantAgent` simplifies the process of building chat interfaces on top of LLMs, handling how user messages and model responses flow, and ensuring all conversation logic is neatly contained in a single agent object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Function Calling Example \n",
    "\n",
    "Function calling in the context of modern LLMs (Large Language Models) lets the model â€œcallâ€ a function (or tool) whenever it detects the userâ€™s query needs that functionâ€™s capabilities. Rather than returning a text-only answer, the LLM returns structured arguments for the function, which your application or framework executes. \\\n",
    "\n",
    "In AutoGenâ€™s AssistantAgent, every function you register is treated as a â€œtoolâ€ that the LLM can invoke. When a user prompt suggests one of those tools is needed, the model produces a structured â€œfunction callâ€ specifying which tool to use and the arguments to pass. The AssistantAgent runs that function in your Python environment, then feeds the result back to the model. Finally, the model incorporates that real-world dataâ€”whether weather forecasts, currency exchanges, or local timesâ€”into its final, user-facing response. This design keeps things neat: the LLM decides what needs doing, and your Python code handles how itâ€™s done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# 1) Define three different \"tool\" functions that your LLM might call.\n",
    "\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Returns a fake weather report for demonstration.\"\"\"\n",
    "    return f\"The current weather in {city} is 25Â°C, sunny.\"\n",
    "\n",
    "async def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Returns a simple stub result for currency conversion.\"\"\"\n",
    "    # In real scenarios, you'd call a currency conversion API here.\n",
    "    # We'll just do a mock 1:1.1 ratio for demonstration.\n",
    "    conversion_rate = 1.1\n",
    "    converted_amount = amount * conversion_rate\n",
    "    return f\"{amount} {from_currency.upper()} is approximately {converted_amount:.2f} {to_currency.upper()}.\"\n",
    "\n",
    "async def get_time_in(tz: str) -> str:\n",
    "    \"\"\"Returns a mock local time for the given timezone.\"\"\"\n",
    "    # You might call a real API or Python library like pytz/dateutil in production.\n",
    "    return f\"The current local time in {tz} is 09:00 AM.\"\n",
    "\n",
    "# 2) Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# 3) Create the minimal Azure OpenAI client.\n",
    "azure_client = AzureOpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 4) Create the AssistantAgent, giving it access to the three tools.\n",
    "#    We mention them in the system_message so the model knows it can call them.\n",
    "agent = AssistantAgent(\n",
    "    name=\"multi_tool_agent\",\n",
    "    model_client=azure_client,\n",
    "    tools=[get_weather, convert_currency, get_time_in],\n",
    "    system_message=(\n",
    "        \"You are a helpful Azure test assistant. You have access to the following tools:\\n\"\n",
    "        \"1) get_weather(city: str) - Provides a weather report.\\n\"\n",
    "        \"2) convert_currency(amount: float, from_currency: str, to_currency: str) - Converts between currencies.\\n\"\n",
    "        \"3) get_time_in(tz: str) - Returns local time in a given timezone.\\n\\n\"\n",
    "        \"Only call these functions if the user is requesting relevant info.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) Letâ€™s define an async function that prompts for multiple tasks.\n",
    "async def demo_multi_tool_calls():\n",
    "    # We'll ask a series of user queries that might trigger each function separately.\n",
    "    user_prompts = [\n",
    "        \"Hi there! Could you tell me the weather in Berlin?\",\n",
    "        \"Actually, could you also convert 100 USD to EUR?\",\n",
    "        \"What's the local time in Tokyo right now?\"\n",
    "    ]\n",
    "\n",
    "    for prompt in user_prompts:\n",
    "        print(f\"---------- user ----------\\n{prompt}\")\n",
    "        # Using run_stream for streaming the conversation to the console\n",
    "        await Console(agent.run_stream(task=prompt))\n",
    "        print(\"\\n\")\n",
    "\n",
    "# 6) Run the demonstration in Jupyter by simply 'await'-ing the function.\n",
    "await demo_multi_tool_calls()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Function Calling with Structured Outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.tools import FunctionTool  # âœ… Correct import for function tools\n",
    "\n",
    "# 1) Apply nest_asyncio (fixes Jupyter asyncio issues)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 2) Define a Pydantic model for structured outputs.\n",
    "class QueryResponse(BaseModel):\n",
    "    weather_report: Optional[str] = None\n",
    "    currency_info: Optional[str] = None\n",
    "    local_time: Optional[str] = None\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "# 3) Define synchronous function tools.\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Returns a mock weather report.\"\"\"\n",
    "    return f\"The current weather in {city} is 25Â°C and sunny.\"\n",
    "\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Performs a mock currency conversion.\"\"\"\n",
    "    rate = 1.1\n",
    "    result = amount * rate\n",
    "    return f\"{amount} {from_currency.upper()} is ~{result:.2f} {to_currency.upper()}.\"\n",
    "\n",
    "def get_time_in(tz: str) -> str:\n",
    "    \"\"\"Returns a mock local time.\"\"\"\n",
    "    return f\"The current local time in {tz} is 09:00 AM.\"\n",
    "\n",
    "# 4) Wrap functions using FunctionTool for structured calling.\n",
    "tool_get_weather = FunctionTool(get_weather, description=\"Fetch current weather for a city.\")\n",
    "tool_convert_currency = FunctionTool(convert_currency, description=\"Convert between currency values.\")\n",
    "tool_get_time_in = FunctionTool(get_time_in, description=\"Retrieve the local time for a given timezone.\")\n",
    "\n",
    "# 5) Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# 6) Create an Azure OpenAI client.\n",
    "azure_client = AzureOpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# 7) Define an AssistantAgent with FunctionTool and enforce JSON output.\n",
    "agent = AssistantAgent(\n",
    "    name=\"multi_tool_agent\",\n",
    "    model_client=azure_client,\n",
    "    tools=[tool_get_weather, tool_convert_currency, tool_get_time_in],\n",
    "    system_message=(\n",
    "        \"You are a helpful Azure AI assistant with access to the following tools:\\n\"\n",
    "        \"1) get_weather(city: str) - Fetches current weather details.\\n\"\n",
    "        \"2) convert_currency(amount: float, from_currency: str, to_currency: str) - Converts currency values.\\n\"\n",
    "        \"3) get_time_in(tz: str) - Retrieves the local time for a given timezone.\\n\\n\"\n",
    "        \"Your responses must be formatted as **valid JSON** with the following fields:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"weather_report\": string or null,\\n'\n",
    "        '  \"currency_info\": string or null,\\n'\n",
    "        '  \"local_time\": string or null,\\n'\n",
    "        '  \"notes\": string or null\\n'\n",
    "        \"}\\n\"\n",
    "        \"You must **ONLY** return JSON, without any additional explanations.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 8) Function for testing multiple tool calls.\n",
    "async def demo_multi_tool_calls():\n",
    "    user_prompts = [\n",
    "        \"What is the weather in Berlin?\",\n",
    "        \"Convert 100 USD to EUR.\",\n",
    "        \"What's the local time in Tokyo?\"\n",
    "    ]\n",
    "\n",
    "    for prompt in user_prompts:\n",
    "        print(f\"---------- User Input ----------\\n{prompt}\")\n",
    "        try:\n",
    "            await Console(agent.run_stream(task=prompt))\n",
    "        except Exception as e:\n",
    "            print(f\"ðŸš¨ Error occurred: {str(e)}\\n\")\n",
    "            print(\"ðŸ”„ Retrying with a simplified query...\")\n",
    "            await Console(agent.run_stream(task=\"Provide the requested data as structured JSON.\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "# 9) Execute the function in a Jupyter cell.\n",
    "await demo_multi_tool_calls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous iteration, which simply allowed an LLM-powered assistant to call basic functions, the updated code enhances reliability, structure, and execution flow. The original version used standard Python functions as tools, but lacked strict enforcement of structured outputs, meaning responses could be inconsistent or require additional post-processing. In contrast, the improved version wraps functions using FunctionTool from autogen_core.tools, ensuring proper argument parsing and structured execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ðŸ‘‰ [You can now proceed with multi-agency capabilities](./part1_autogen_multi_agent.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
