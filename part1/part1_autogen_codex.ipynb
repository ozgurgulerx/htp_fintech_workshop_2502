{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building with AutoGen \n",
    "### Please Complete the **[environment setup](part1_environment_setup.ipynb)** first before progressing with the notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Starting Building with AutoGen \n",
    "Let's start with a simple example of how AutoGen is integrated Azure OpenAI. \n",
    "The example will confirm Azure OpenAI endpoints and keys are correctly configured and that AutoGen is installed and working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Create a minimal Azure OpenAI client.\n",
    "azure_client = AzureOpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Create an assistant agent with a simple system message.\n",
    "agent = AssistantAgent(\n",
    "    name=\"azure_test_agent\",\n",
    "    model_client=azure_client,\n",
    "    system_message=\"You are a helpful Azure test assistant.\"\n",
    ")\n",
    "\n",
    "async def test_connection():\n",
    "    user_input = \"Hello! Are you working?\"\n",
    "    await Console(agent.run_stream(task=user_input))\n",
    "\n",
    "await test_connection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **autogen** framework provides multiple agent roles—such as `AssistantAgent`, `UserAgent`, `FunctionAgent` (for function or tool calls), and `MultiAgent` (for orchestrating multiple participants)—each designed for specialized behaviors in a conversation. For example:\n",
    "\n",
    "- `AssistantAgent`: Represents a helpful AI assistant.\n",
    "- `FunctionAgent`: Handles specific tools or API calls.\n",
    "- `MultiAgent`: Coordinates interactions among different agents.\n",
    "\n",
    "This modular design makes it straightforward to build and manage multi-turn, multi-role dialogues, assigning clear, distinct responsibilities to each agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AssistantAgent` we used here is a high-level wrapper around a language model client (such as an AzureOpenAI model) that manages:\n",
    "\n",
    "1. **Conversational Context**: It keeps track of all previous exchanges and organizes them into a coherent dialog.\n",
    "2. **System or Instruction Prompt**: You can give it a “system message” that defines the overarching instructions or persona, such as:\n",
    "   > \"You are a helpful Azure test assistant.\"\n",
    "\n",
    "By consolidating these elements, `AssistantAgent` simplifies the process of building chat interfaces on top of LLMs, handling how user messages and model responses flow, and ensuring all conversation logic is neatly contained in a single agent object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Function Calling Example \n",
    "\n",
    "Function calling in the context of modern LLMs (Large Language Models) lets the model “call” a function (or tool) whenever it detects the user’s query needs that function’s capabilities. Rather than returning a text-only answer, the LLM returns structured arguments for the function, which your application or framework executes. \\\n",
    "\n",
    "In AutoGen’s AssistantAgent, every function you register is treated as a “tool” that the LLM can invoke. When a user prompt suggests one of those tools is needed, the model produces a structured “function call” specifying which tool to use and the arguments to pass. The AssistantAgent runs that function in your Python environment, then feeds the result back to the model. Finally, the model incorporates that real-world data—whether weather forecasts, currency exchanges, or local times—into its final, user-facing response. This design keeps things neat: the LLM decides what needs doing, and your Python code handles how it’s done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# 1) Define three different \"tool\" functions that your LLM might call.\n",
    "\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Returns a fake weather report for demonstration.\"\"\"\n",
    "    return f\"The current weather in {city} is 25°C, sunny.\"\n",
    "\n",
    "async def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Returns a simple stub result for currency conversion.\"\"\"\n",
    "    # In real scenarios, you'd call a currency conversion API here.\n",
    "    # We'll just do a mock 1:1.1 ratio for demonstration.\n",
    "    conversion_rate = 1.1\n",
    "    converted_amount = amount * conversion_rate\n",
    "    return f\"{amount} {from_currency.upper()} is approximately {converted_amount:.2f} {to_currency.upper()}.\"\n",
    "\n",
    "async def get_time_in(tz: str) -> str:\n",
    "    \"\"\"Returns a mock local time for the given timezone.\"\"\"\n",
    "    # You might call a real API or Python library like pytz/dateutil in production.\n",
    "    return f\"The current local time in {tz} is 09:00 AM.\"\n",
    "\n",
    "# 2) Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# 3) Create the minimal Azure OpenAI client.\n",
    "azure_client = AzureOpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 4) Create the AssistantAgent, giving it access to the three tools.\n",
    "#    We mention them in the system_message so the model knows it can call them.\n",
    "agent = AssistantAgent(\n",
    "    name=\"multi_tool_agent\",\n",
    "    model_client=azure_client,\n",
    "    tools=[get_weather, convert_currency, get_time_in],\n",
    "    system_message=(\n",
    "        \"You are a helpful Azure test assistant. You have access to the following tools:\\n\"\n",
    "        \"1) get_weather(city: str) - Provides a weather report.\\n\"\n",
    "        \"2) convert_currency(amount: float, from_currency: str, to_currency: str) - Converts between currencies.\\n\"\n",
    "        \"3) get_time_in(tz: str) - Returns local time in a given timezone.\\n\\n\"\n",
    "        \"Only call these functions if the user is requesting relevant info.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) Let’s define an async function that prompts for multiple tasks.\n",
    "async def demo_multi_tool_calls():\n",
    "    # We'll ask a series of user queries that might trigger each function separately.\n",
    "    user_prompts = [\n",
    "        \"Hi there! Could you tell me the weather in Berlin?\",\n",
    "        \"Actually, could you also convert 100 USD to EUR?\",\n",
    "        \"What's the local time in Tokyo right now?\"\n",
    "    ]\n",
    "\n",
    "    for prompt in user_prompts:\n",
    "        print(f\"---------- user ----------\\n{prompt}\")\n",
    "        # Using run_stream for streaming the conversation to the console\n",
    "        await Console(agent.run_stream(task=prompt))\n",
    "        print(\"\\n\")\n",
    "\n",
    "# 6) Run the demonstration in Jupyter by simply 'await'-ing the function.\n",
    "await demo_multi_tool_calls()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Function Calling with Structured Outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- User Input ----------\n",
      "What is the weather in Berlin?\n",
      "---------- user ----------\n",
      "What is the weather in Berlin?\n",
      "---------- multi_tool_agent ----------\n",
      "[FunctionCall(id='call_CD7ctTezPk3XPaAseMnMnl0y', arguments='{\"city\":\"Berlin\"}', name='get_weather')]\n",
      "---------- multi_tool_agent ----------\n",
      "[FunctionExecutionResult(content='The current weather in Berlin is 25°C and sunny.', call_id='call_CD7ctTezPk3XPaAseMnMnl0y', is_error=False)]\n",
      "---------- multi_tool_agent ----------\n",
      "The current weather in Berlin is 25°C and sunny.\n",
      "\n",
      "\n",
      "---------- User Input ----------\n",
      "Convert 100 USD to EUR.\n",
      "---------- user ----------\n",
      "Convert 100 USD to EUR.\n",
      "🚨 Error occurred: Error code: 500 - {'error': {'message': 'The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.', 'type': 'model_error', 'param': None, 'code': None}}\n",
      "\n",
      "🔄 Retrying with a simplified query...\n",
      "---------- user ----------\n",
      "Provide the requested data as structured JSON.\n",
      "---------- multi_tool_agent ----------\n",
      "[FunctionCall(id='call_xWu08cbk33IoPXetLJzxhbYo', arguments='{\"city\": \"Berlin\"}', name='get_weather'), FunctionCall(id='call_WGcIRs3WKjd3t8LtPdLJ6W3n', arguments='{\"amount\": 100, \"from_currency\": \"USD\", \"to_currency\": \"EUR\"}', name='convert_currency')]\n",
      "---------- multi_tool_agent ----------\n",
      "[FunctionExecutionResult(content='The current weather in Berlin is 25°C and sunny.', call_id='call_xWu08cbk33IoPXetLJzxhbYo', is_error=False), FunctionExecutionResult(content='100.0 USD is ~110.00 EUR.', call_id='call_WGcIRs3WKjd3t8LtPdLJ6W3n', is_error=False)]\n",
      "---------- multi_tool_agent ----------\n",
      "The current weather in Berlin is 25°C and sunny.\n",
      "100.0 USD is ~110.00 EUR.\n",
      "\n",
      "\n",
      "---------- User Input ----------\n",
      "What's the local time in Tokyo?\n",
      "---------- user ----------\n",
      "What's the local time in Tokyo?\n",
      "🚨 Error occurred: Error code: 500 - {'error': {'message': 'The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.', 'type': 'model_error', 'param': None, 'code': None}}\n",
      "\n",
      "🔄 Retrying with a simplified query...\n",
      "---------- user ----------\n",
      "Provide the requested data as structured JSON.\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': {'message': 'The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.', 'type': 'model_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 9) Execute the function in a Jupyter cell.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m demo_multi_tool_calls()\n",
      "Cell \u001b[0;32mIn[36], line 90\u001b[0m, in \u001b[0;36mdemo_multi_tool_calls\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚨 Error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Retrying with a simplified query...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m Console(agent\u001b[38;5;241m.\u001b[39mrun_stream(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide the requested data as structured JSON.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py:117\u001b[0m, in \u001b[0;36mConsole\u001b[0;34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m last_processed: Optional[T] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[1;32m    119\u001b[0m         duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/autogen_agentchat/agents/_base_chat_agent.py:176\u001b[0m, in \u001b[0;36mBaseChatAgent.run_stream\u001b[0;34m(self, task, cancellation_token)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages_stream(input_messages, cancellation_token):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m message\u001b[38;5;241m.\u001b[39mchat_message\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:416\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages_stream\u001b[0;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_result, CreateResult)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    417\u001b[0m         llm_messages, tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tools \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handoff_tools, cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token\n\u001b[1;32m    418\u001b[0m     )\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Add the response to the model context.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_context\u001b[38;5;241m.\u001b[39madd_message(AssistantMessage(content\u001b[38;5;241m=\u001b[39mmodel_result\u001b[38;5;241m.\u001b[39mcontent, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:534\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient.create\u001b[0;34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    533\u001b[0m     cancellation_token\u001b[38;5;241m.\u001b[39mlink_future(future)\n\u001b[0;32m--> 534\u001b[0m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_beta_client:\n\u001b[1;32m    536\u001b[0m     result \u001b[38;5;241m=\u001b[39m cast(ParsedChatCompletion[Any], result)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py:289\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:385\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py:202\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1927\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1887\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1925\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1926\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1929\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1930\u001b[0m             {\n\u001b[1;32m   1931\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1934\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1935\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1936\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1937\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1938\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1939\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1940\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1941\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1943\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1944\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1949\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1950\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1951\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1952\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1953\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1954\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1955\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1956\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1957\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1958\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1959\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1960\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1961\u001b[0m             },\n\u001b[1;32m   1962\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1963\u001b[0m         ),\n\u001b[1;32m   1964\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1965\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1966\u001b[0m         ),\n\u001b[1;32m   1967\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1968\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1969\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1970\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/openai/_base_client.py:1856\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1843\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1844\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1853\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1854\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1855\u001b[0m     )\n\u001b[0;32m-> 1856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/openai/_base_client.py:1550\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1548\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1551\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1552\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1553\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1554\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1555\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1556\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/openai/_base_client.py:1636\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1637\u001b[0m         input_options,\n\u001b[1;32m   1638\u001b[0m         cast_to,\n\u001b[1;32m   1639\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1640\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1641\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1642\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1643\u001b[0m     )\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/openai/_base_client.py:1683\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1679\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1684\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1685\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1686\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1687\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1688\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1689\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/openai/_base_client.py:1636\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1637\u001b[0m         input_options,\n\u001b[1;32m   1638\u001b[0m         cast_to,\n\u001b[1;32m   1639\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1640\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1641\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1642\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1643\u001b[0m     )\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/openai/_base_client.py:1683\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1679\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1684\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1685\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1686\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1687\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1688\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1689\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Projects/htp_fintech_workshop_2502/venv/lib/python3.12/site-packages/openai/_base_client.py:1651\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1648\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1650\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1651\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1654\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1655\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1660\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'error': {'message': 'The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.', 'type': 'model_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.tools import FunctionTool  # ✅ Correct import for function tools\n",
    "\n",
    "# 1) Apply nest_asyncio (fixes Jupyter asyncio issues)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 2) Define a Pydantic model for structured outputs.\n",
    "class QueryResponse(BaseModel):\n",
    "    weather_report: Optional[str] = None\n",
    "    currency_info: Optional[str] = None\n",
    "    local_time: Optional[str] = None\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "# 3) Define synchronous function tools.\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Returns a mock weather report.\"\"\"\n",
    "    return f\"The current weather in {city} is 25°C and sunny.\"\n",
    "\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Performs a mock currency conversion.\"\"\"\n",
    "    rate = 1.1\n",
    "    result = amount * rate\n",
    "    return f\"{amount} {from_currency.upper()} is ~{result:.2f} {to_currency.upper()}.\"\n",
    "\n",
    "def get_time_in(tz: str) -> str:\n",
    "    \"\"\"Returns a mock local time.\"\"\"\n",
    "    return f\"The current local time in {tz} is 09:00 AM.\"\n",
    "\n",
    "# 4) Wrap functions using FunctionTool for structured calling.\n",
    "tool_get_weather = FunctionTool(get_weather, description=\"Fetch current weather for a city.\")\n",
    "tool_convert_currency = FunctionTool(convert_currency, description=\"Convert between currency values.\")\n",
    "tool_get_time_in = FunctionTool(get_time_in, description=\"Retrieve the local time for a given timezone.\")\n",
    "\n",
    "# 5) Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# 6) Create an Azure OpenAI client.\n",
    "azure_client = AzureOpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# 7) Define an AssistantAgent with FunctionTool and enforce JSON output.\n",
    "agent = AssistantAgent(\n",
    "    name=\"multi_tool_agent\",\n",
    "    model_client=azure_client,\n",
    "    tools=[tool_get_weather, tool_convert_currency, tool_get_time_in],\n",
    "    system_message=(\n",
    "        \"You are a helpful Azure AI assistant with access to the following tools:\\n\"\n",
    "        \"1) get_weather(city: str) - Fetches current weather details.\\n\"\n",
    "        \"2) convert_currency(amount: float, from_currency: str, to_currency: str) - Converts currency values.\\n\"\n",
    "        \"3) get_time_in(tz: str) - Retrieves the local time for a given timezone.\\n\\n\"\n",
    "        \"Your responses must be formatted as **valid JSON** with the following fields:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"weather_report\": string or null,\\n'\n",
    "        '  \"currency_info\": string or null,\\n'\n",
    "        '  \"local_time\": string or null,\\n'\n",
    "        '  \"notes\": string or null\\n'\n",
    "        \"}\\n\"\n",
    "        \"You must **ONLY** return JSON, without any additional explanations.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 8) Function for testing multiple tool calls.\n",
    "async def demo_multi_tool_calls():\n",
    "    user_prompts = [\n",
    "        \"What is the weather in Berlin?\",\n",
    "        \"Convert 100 USD to EUR.\",\n",
    "        \"What's the local time in Tokyo?\"\n",
    "    ]\n",
    "\n",
    "    for prompt in user_prompts:\n",
    "        print(f\"---------- User Input ----------\\n{prompt}\")\n",
    "        try:\n",
    "            await Console(agent.run_stream(task=prompt))\n",
    "        except Exception as e:\n",
    "            print(f\"🚨 Error occurred: {str(e)}\\n\")\n",
    "            print(\"🔄 Retrying with a simplified query...\")\n",
    "            await Console(agent.run_stream(task=\"Provide the requested data as structured JSON.\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "# 9) Execute the function in a Jupyter cell.\n",
    "await demo_multi_tool_calls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous iteration, which simply allowed an LLM-powered assistant to call basic functions, the updated code enhances reliability, structure, and execution flow. The original version used standard Python functions as tools, but lacked strict enforcement of structured outputs, meaning responses could be inconsistent or require additional post-processing. In contrast, the improved version wraps functions using FunctionTool from autogen_core.tools, ensuring proper argument parsing and structured execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "👉 [You can now proceed with coding](./part1_autogen_multi_agent.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
