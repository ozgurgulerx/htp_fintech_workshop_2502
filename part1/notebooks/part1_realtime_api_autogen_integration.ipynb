{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Driven Call Center with OpenAI Real-Time Voice and AutoGen\n",
    "\n",
    "Building an AI-driven call center involves combining real-time voice processing, AI-powered conversation management, call flow logic and a scalable deployment strategy.\n",
    "\n",
    "## AI-Driven Conversation Handling with AutoGen\n",
    "\n",
    "AutoGen and the Real-Time API are a powerful couple for AI-powered call centers because AutoGen can act as the intelligent backbone while the Real-Time API provides the seamless user interface. AutoGen decides how to handle each caller’s request—whether that means pulling data from external APIs, generating code, or delegating tasks to specialized sub-agents—while the Real-Time API continuously captures and transcribes the caller’s speech, returning AutoGen’s responses as synthesized audio in real time. This orchestrated approach gives call centers a flexible, voice-driven flow, where callers simply talk and hear spoken answers, while AutoGen invisibly coordinates the logic, function calling, and data retrieval in the background.\n",
    "\n",
    "AutoGen and the Real-Time API fit together by letting the Real-Time API handle live audio streaming, while AutoGen orchestrates the AI logic behind the scenes. A typical integration might look like this: First, you set up a Real-Time API WebSocket session (as in the basic example), which continuously listens to the user’s microphone and returns transcribed text. Next, that text goes to your AutoGen orchestrator, which manages a set of specialized agents—each with particular “skills.” AutoGen analyzes the user’s request, decides which agent or function to invoke (for example, pulling data from a third-party API, generating code, or performing a custom task), and collects the results. AutoGen then assembles a final response—usually in text form—and hands it back to the Real-Time API. Finally, the Real-Time API converts that response into synthesized speech and streams it to the caller. Throughout the conversation, AutoGen can juggle multiple agents, access different APIs, and keep track of context, while the Real-Time API manages the user-facing audio input and output in real time. This division of responsibilities keeps things clear: the Real-Time API is effectively the “UI layer,” and AutoGen is the “automation backbone” that decides what to do with each request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import websockets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "##############################\n",
    "# 1) AUTO-GEN ORCHESTRATOR\n",
    "##############################\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "\n",
    "class WeatherAgent(AssistantAgent):\n",
    "    \"\"\"Simplified agent that returns weather data.\"\"\"\n",
    "    def handle_custom(self, user_text: str) -> str:\n",
    "        return \"It’s 22°C and sunny in Istanbul.\"\n",
    "\n",
    "class CodeAgent(AssistantAgent):\n",
    "    \"\"\"Simplified agent that returns sample Python code.\"\"\"\n",
    "    def handle_custom(self, user_text: str) -> str:\n",
    "        return (\n",
    "            \"def greet(name):\\n\"\n",
    "            \"    return f'Hello, {name}!'\"\n",
    "        )\n",
    "\n",
    "class AutoGenOrchestrator:\n",
    "    \"\"\"\n",
    "    Orchestrator that routes user queries to the appropriate agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, azure_client):\n",
    "        self.weather_agent = WeatherAgent(\n",
    "            name=\"WeatherAgent\",\n",
    "            model_client=azure_client,\n",
    "            system_message=\"You are a weather assistant.\"\n",
    "        )\n",
    "        self.code_agent = CodeAgent(\n",
    "            name=\"CodeAgent\",\n",
    "            model_client=azure_client,\n",
    "            system_message=\"You are a code-generating assistant.\"\n",
    "        )\n",
    "\n",
    "    async def handle_user_text(self, user_text: str) -> str:\n",
    "        if \"weather\" in user_text.lower():\n",
    "            return await self._run_agent(self.weather_agent, user_text)\n",
    "        elif \"code\" in user_text.lower():\n",
    "            return await self._run_agent(self.code_agent, user_text)\n",
    "        else:\n",
    "            return \"Try asking for weather or code?\"\n",
    "\n",
    "    async def _run_agent(self, agent: AssistantAgent, user_text: str) -> str:\n",
    "        final_text = agent.handle_custom(user_text)\n",
    "        return final_text\n",
    "\n",
    "##############################\n",
    "# 2) AUDIO PROCESSING (Real-Time)\n",
    "##############################\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"Handles audio input, buffering, and detecting interruptions.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=24000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.vad_threshold = 0.015\n",
    "        self.interrupt_threshold = 0.02\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.min_speech_duration = int(0.3 * sample_rate)\n",
    "        self.max_silence_duration = int(0.8 * sample_rate)\n",
    "        self.main_buffer = []\n",
    "        self.interrupt_buffer = []\n",
    "        self.is_speaking = False\n",
    "        self.speech_detected = False\n",
    "        self.is_interrupting = False\n",
    "\n",
    "    def process_audio(self, indata):\n",
    "        audio_level = np.abs(indata).mean() / 32768.0\n",
    "        if self.is_speaking and audio_level > self.interrupt_threshold:\n",
    "            self.is_interrupting = True\n",
    "            self.interrupt_buffer.extend(indata.tobytes())\n",
    "            return\n",
    "        if self.is_interrupting:\n",
    "            self.interrupt_buffer.extend(indata.tobytes())\n",
    "            return\n",
    "        if not self.is_speaking:\n",
    "            if audio_level > self.vad_threshold:\n",
    "                self.speech_detected = True\n",
    "                self.speech_frames += len(indata)\n",
    "                self.silence_frames = 0\n",
    "                self.main_buffer.extend(indata.tobytes())\n",
    "            elif self.speech_detected:\n",
    "                self.silence_frames += len(indata)\n",
    "                if self.silence_frames < self.max_silence_duration:\n",
    "                    self.main_buffer.extend(indata.tobytes())\n",
    "\n",
    "    def check_interruption(self):\n",
    "        return self.is_interrupting\n",
    "\n",
    "    def get_interrupt_audio(self):\n",
    "        if not self.interrupt_buffer:\n",
    "            return None\n",
    "        audio_data = bytes(self.interrupt_buffer)\n",
    "        self.interrupt_buffer.clear()\n",
    "        self.is_interrupting = False\n",
    "        return audio_data\n",
    "\n",
    "    def should_process(self):\n",
    "        return (self.speech_detected and\n",
    "                self.speech_frames >= self.min_speech_duration and\n",
    "                self.silence_frames >= self.max_silence_duration)\n",
    "\n",
    "    def reset(self):\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.speech_detected = False\n",
    "        audio_data = bytes(self.main_buffer)\n",
    "        self.main_buffer.clear()\n",
    "        return audio_data\n",
    "\n",
    "\n",
    "##############################\n",
    "# 3) CONVERSATION SYSTEM\n",
    "##############################\n",
    "\n",
    "class ConversationSystem:\n",
    "    \"\"\"\n",
    "    - Connects to Azure Real-Time for 2-way audio streaming.\n",
    "    - On receiving recognized text from the user, we hand it off to AutoGenOrchestrator.\n",
    "    - Then we take AutoGen's text response and request Azure to produce spoken audio output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, orchestrator: AutoGenOrchestrator):\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found\")\n",
    "\n",
    "        # Hardcoded WebSocket URL\n",
    "        self.url = (\n",
    "            \"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            \"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview\"\n",
    "        )\n",
    "\n",
    "        self.audio_processor = AudioProcessor()\n",
    "        self.streams = {'input': None, 'output': None}\n",
    "        self.orchestrator = orchestrator\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        import sounddevice as sd\n",
    "        self.streams['output'] = sd.OutputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16\n",
    "        )\n",
    "        self.streams['input'] = sd.InputStream(\n",
    "            samplerate=24000, channels=1, dtype=np.int16,\n",
    "            callback=self.audio_callback, blocksize=4800\n",
    "        )\n",
    "        for stream in self.streams.values():\n",
    "            stream.start()\n",
    "\n",
    "    async def send_audio_to_azure(self, websocket, audio_data: bytes):\n",
    "        audio_b64 = base64.b64encode(audio_data).decode('utf-8')\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": audio_b64\n",
    "        }))\n",
    "        await websocket.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"Main conversation loop: set up audio, connect to Real-Time, and wait for user speech.\"\"\"\n",
    "        await self.setup_audio()\n",
    "        print(\"Audio setup complete. Connecting to Real-Time...\")\n",
    "\n",
    "        async with websockets.connect(self.url) as ws:\n",
    "            print(\"Connected to Azure Real-Time API.\")\n",
    "\n",
    "            while True:\n",
    "                if self.audio_processor.should_process():\n",
    "                    audio_data = self.audio_processor.reset()\n",
    "                    await self.send_audio_to_azure(ws, audio_data)\n",
    "\n",
    "                await asyncio.sleep(0.05)\n",
    "\n",
    "\n",
    "##############################\n",
    "# 4) Putting It All Together\n",
    "##############################\n",
    "\n",
    "async def main():\n",
    "    load_dotenv()\n",
    "    azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    azure_client = AzureOpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        api_version=\"2024-06-01\",\n",
    "        azure_endpoint=\"https://aoai-ep-swedencentral02.openai.azure.com\",\n",
    "        api_key=azure_api_key\n",
    "    )\n",
    "\n",
    "    orchestrator = AutoGenOrchestrator(azure_client)\n",
    "    system = ConversationSystem(orchestrator)\n",
    "    await system.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
