{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Effective User Agents with Azure OpenAI’s Real-Time API \n",
    "Developing a responsive, intelligent user agent using Azure OpenAI’s real-time capabilities involves careful attention to voice interactions, context retention, performance, and robust design. \n",
    "\n",
    "Key challanges include...\n",
    "\n",
    "## 1. Voice Interruption Handling\n",
    "\n",
    "Our goal is to handle user \"barge-in\"'s gracefully so the AI stops talking and immediately re-focuses on the new user input. By default, Azure OpenAI Real-Time API offers built-in Voice Activity Detection (VAD), which you can tune for your environment. If your setting is noisy, raising the VAD threshold helps avoid false triggers; if it’s too sensitive, lower it. For extreme conditions or custom scenarios, you can disable built-in VAD and implement your own, manually stopping TTS playback whenever you detect a genuine interruption.\n",
    "\n",
    "Once an interruption occurs, be sure to truncate the unfinished response on the server side, so the AI doesn’t keep referencing text the user never heard. This keeps the conversation strictly aligned with what was actually said. A helpful touch is to acknowledge the interruption with a brief apology or confirmation—users will find the experience more natural. If barge-ins happen a lot, add a small “debounce” window so quick background sounds don’t falsely trigger interruptions.\n",
    "\n",
    "### Automatic VAD (Voice Activity Detection)\n",
    "- Use built-in VAD from Azure OpenAI Real-Time to detect interruptions.\n",
    "- Adjust VAD sensitivity (threshold, padding) to reduce false positives in noisy settings.\n",
    "- Truncation for Synchronization: When the user interrupts mid-response, call the API to truncate the remainder of the AI’s reply.\n",
    "This ensures the agent doesn’t assume it said something the user never heard, keeping the conversation “in sync.”\n",
    "Optimization Tips:\n",
    "\n",
    "There is usually more optimisation to be done as per your environment:\n",
    "- Fine-Tune VAD: Adjust thresholds to match your environment (e.g. set higher for busy call centers).\n",
    "- Detect Early: Implement a short “debounce” so minor noises don’t trigger interruption, but user speech does.\n",
    "- Acknowledge Interruptions: Provide a quick apology or relevant acknowledgment to make it feel more natural.\n",
    "\n",
    "\n",
    "## 2. Context Management\n",
    "\n",
    "We want our AI to feel like it genuinely remembers what’s been said, even across multiple turns. A good approach is to maintain a rolling conversation history by storing recent messages—both user and assistant—and bundling them into each API call. If the session drags on, you can summarize older content to keep token usage manageable.\n",
    "\n",
    "At the start of any session, it helps to present a system prompt describing the assistant’s role, tone, and any user-specific details. This keeps the AI focused and aligned with your goals. For extended or returning conversations, stash key data points in an external store—like a database or vector index—to fetch them on demand, especially if the user references something from a previous day.\n",
    "\n",
    "To optimize context usage, consider summarizing older exchanges so the assistant doesn’t forget important details. Always label who is speaking (e.g. “User,” “Assistant”) so the model can track conversation flow. And if the user abruptly pivots to a completely new topic, feel free to reset the context or at least partially clear it to avoid confusion.\n",
    "\n",
    "## 3. Latency Optimization\n",
    "\n",
    "We want the conversation to feel immediate and responsive, so adopting a streaming architecture is key. Let Azure OpenAI process user audio in real time and start sending back partial responses as soon as they’re generated—no waiting for a full transcription. To keep it efficient, maintain a persistent WebSocket so you don’t have to reconnect on every turn, and consider pooling connections if you need to handle many sessions in parallel.\n",
    "\n",
    "Whenever possible, go for an end-to-end pipeline by using Real-Time audio input/output directly, avoiding separate STT/TTS services that can add latency. Hosting your service in the same Azure region as your OpenAI resource also helps keep roundtrip times low.\n",
    "\n",
    "For further optimization, reduce the number of layers in your architecture—fewer services in between means quicker responses—and send smaller audio chunks more frequently to maintain that “streaming” feel. Finally, don’t forget to load test your setup under realistic conditions so you can spot bottlenecks and fine-tune performance before going live.\n",
    "\n",
    "## 4. Speaker Identification & Differentiation\n",
    "\n",
    "Although we will not use diarization in this workshop it is an important concept to mention while building Voice UI's. \n",
    "When multiple people are talking, it’s easy for your AI to lose track of who said what. Speaker diarization—labeling each voice as “Speaker 1,” “Speaker 2,” and so on—keeps everything straight, especially if you feed these labels back into the conversation context. If you need real identities, you can enroll voice profiles (until that feature is retired), but simple on-the-fly diarization often works fine.\n",
    "\n",
    "In your prompts, clearly denote each speaker (like “Alice: … Bob: …”) so the model can address or reference them accurately. For instance, if “Alice” asked a question, the AI knows to respond directly to her. If your application can eventually match “Speaker 1” to Alice, rename labels for clarity. And if folks talk over each other, it’s wise to pause or ask for clarification. Finally, consider multi-user memory—persist speaker-specific context or preferences so the AI can recall each person’s details even across multiple sessions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWO-WAY AUDIO CONVERSATION v1.0\n",
    "\n",
    "Now review & run the script part1_realtime_api_advanced_converse_step1.py in the repo. \n",
    "\n",
    "This code sets up a two-way audio conversation with Azure OpenAI’s Real-Time API. You speak into your microphone; the audio is sent to Azure in near-real-time. The API processes that audio and returns both text and audio replies, which the script then plays back through your speakers.\n",
    "\n",
    "In essence: It’s a basic, end-to-end voice assistant loop—your microphone feeds Azure GPT-4o Real-Time; the model replies with generated speech you can hear.\n",
    "\n",
    "This is what our code does...\n",
    "- **Audio Setup** : It sets up an input stream for your microphone and an output stream for your speakers (both at 24 kHz).\n",
    "Any audio captured gets temporarily buffered—unless the script is busy playing back the AI’s voice, in which case it ignores incoming sound to avoid echo/feedback.\n",
    "- **Session Configuration** : Upon connecting, it sends a “session.update” message specifying things like the voice to use (e.g. “alloy”), VAD thresholds, and the instruction text (“You are a helpful AI assistant…”).\n",
    "- **Conversation Loop** : First it sends a text greeting (“Hello”) to get a response from the AI.\n",
    "Then it repeatedly waits two seconds, grabs whatever audio you spoke into the mic, and sends that audio (Base64-encoded) to the Real-Time API.\n",
    "It requests a reply in both text and audio form, then streams back the AI’s spoken response in real time.\n",
    "- **Playback & Barge-In** :For each chunk of audio data received from the API, it decodes and plays it over your speakers.\n",
    "There’s a simple voice activity detection (VAD) setup, but it doesn’t attempt to handle mid-response interruptions—just detects silence to know when you’re done speaking.\n",
    "- **Ending Conditions** :The loop continues indefinitely unless something goes wrong (like a network error) or you terminate the script. Each full cycle of sending audio and receiving AI output counts as a single turn in the conversation.\n",
    "\n",
    "Overall, it’s a straightforward real-time audio pipeline: (1) capture mic audio → (2) send to Azure → (3) get response audio → (4) play it back. It’s a practical demo of how to create a voice-first GPT-style assistant with Azure’s Real-Time API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import websockets\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "class ConversationSystem:\n",
    "    def __init__(self):\n",
    "        # Load environment variables (including AZURE_OPENAI_API_KEY)\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Initialize audio streams and flags\n",
    "        self.input_stream = None\n",
    "        self.output_stream = None\n",
    "        self.is_speaking = False\n",
    "        self.input_buffer = []\n",
    "        \n",
    "        # Retrieve your Azure OpenAI key from environment\n",
    "        self.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"AZURE_OPENAI_API_KEY not found in environment\")\n",
    "        \n",
    "        # Construct the WebSocket URL for Azure OpenAI Real-Time\n",
    "        self.url = (\n",
    "            f\"wss://aoai-ep-swedencentral02.openai.azure.com/openai/realtime?\"\n",
    "            f\"api-version=2024-10-01-preview&deployment=gpt-4o-realtime-preview&api-key={self.api_key}\"\n",
    "        )\n",
    "\n",
    "    async def setup_audio(self):\n",
    "        \"\"\"\n",
    "        Sets up audio input (microphone) and output (speakers) streams at 24kHz,\n",
    "        using the 'sounddevice' library. \n",
    "        \"\"\"\n",
    "        print(\"Setting up audio streams...\")\n",
    "        \n",
    "        # Output stream: for playing back AI responses\n",
    "        self.output_stream = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n",
    "        \n",
    "        # Input stream: captures user speech via microphone\n",
    "        self.input_stream = sd.InputStream(\n",
    "            samplerate=24000, \n",
    "            channels=1, \n",
    "            dtype=np.int16,\n",
    "            callback=self.audio_callback\n",
    "        )\n",
    "        \n",
    "        # Start both streams\n",
    "        self.output_stream.start()\n",
    "        self.input_stream.start()\n",
    "        print(\"Audio streams started\")\n",
    "        \n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        \"\"\"\n",
    "        Called whenever there's new audio data from the mic. \n",
    "        If we're not currently playing back the AI's response, \n",
    "        we buffer this audio for sending to Azure.\n",
    "        \"\"\"\n",
    "        if status:\n",
    "            print(f\"Input stream error: {status}\")\n",
    "        # Only record mic input if we're not playing AI audio\n",
    "        if not self.is_speaking:\n",
    "            self.input_buffer.extend(indata.tobytes())\n",
    "\n",
    "    async def start_conversation(self):\n",
    "        \"\"\"\n",
    "        Main loop to manage WebSocket connection and audio round-trips:\n",
    "        - Connect to Azure Real-Time\n",
    "        - Send an initial text message\n",
    "        - Continuously record mic audio (2-second chunks) and send to Azure\n",
    "        - Request the AI's response and play it back\n",
    "        \"\"\"\n",
    "        try:\n",
    "            async with websockets.connect(self.url) as ws:\n",
    "                print(\"Connected to WebSocket\")\n",
    "                \n",
    "                # Configure session with voice, instructions, VAD, etc.\n",
    "                await self.setup_session(ws)\n",
    "                \n",
    "                # Send a simple greeting as initial text\n",
    "                await self.send_message(ws, \"Hello\")\n",
    "                await self.handle_response(ws)\n",
    "\n",
    "                # Repeatedly capture short audio segments, then request a reply\n",
    "                while True:\n",
    "                    print(\"\\nListening... (speak for at least 2 seconds)\")\n",
    "                    self.input_buffer.clear()\n",
    "                    \n",
    "                    # Wait 2 seconds to accumulate mic input\n",
    "                    await asyncio.sleep(2)\n",
    "\n",
    "                    # If there's any recorded audio, send it to the Real-Time API\n",
    "                    if len(self.input_buffer) > 0:\n",
    "                        print(\"Processing your input...\")\n",
    "                        audio_data = bytes(self.input_buffer)\n",
    "                        self.input_buffer.clear()\n",
    "                        \n",
    "                        # Convert audio to Base64\n",
    "                        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n",
    "                        \n",
    "                        # Append new audio data to the server-side audio buffer\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.append\",\n",
    "                            \"audio\": base64_audio\n",
    "                        }))\n",
    "                        \n",
    "                        # Signal that we've finished sending this chunk of audio\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.commit\"\n",
    "                        }))\n",
    "                        \n",
    "                        # Request both audio and text back from Azure\n",
    "                        await ws.send(json.dumps({\n",
    "                            \"type\": \"response.create\",\n",
    "                            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "                        }))\n",
    "                        \n",
    "                        # Handle AI's response (streaming audio)\n",
    "                        await self.handle_response(ws)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in conversation: {e}\")\n",
    "\n",
    "    async def setup_session(self, ws):\n",
    "        \"\"\"\n",
    "        Sends a session update payload to configure voice, instructions, \n",
    "        and VAD settings, then waits for 'session.created' to confirm.\n",
    "        \"\"\"\n",
    "        session_payload = {\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": \"alloy\",\n",
    "                \"instructions\": \"You are a helpful AI assistant. Keep responses brief and engaging.\",\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.5,\n",
    "                    \"prefix_padding_ms\": 300,\n",
    "                    \"silence_duration_ms\": 200\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await ws.send(json.dumps(session_payload))\n",
    "        \n",
    "        # Wait for confirmation or error\n",
    "        while True:\n",
    "            response = await ws.recv()\n",
    "            data = json.loads(response)\n",
    "            if data.get(\"type\") == \"session.created\":\n",
    "                print(\"Session setup complete\")\n",
    "                break\n",
    "            elif data.get(\"type\") == \"error\":\n",
    "                raise Exception(\"Error creating session\")\n",
    "\n",
    "    async def send_message(self, ws, text):\n",
    "        \"\"\"\n",
    "        Sends a user message in text form, then requests an audio+text response.\n",
    "        \"\"\"\n",
    "        message_payload = {\n",
    "            \"type\": \"conversation.item.create\",\n",
    "            \"item\": {\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": text}]\n",
    "            }\n",
    "        }\n",
    "        # Post the user message\n",
    "        await ws.send(json.dumps(message_payload))\n",
    "        \n",
    "        # Ask Azure to generate a response in audio + text\n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\"modalities\": [\"audio\", \"text\"]}\n",
    "        }))\n",
    "\n",
    "    async def handle_response(self, ws):\n",
    "        \"\"\"\n",
    "        Reads streamed audio chunks from Azure (Base64-encoded deltas),\n",
    "        decodes them, and plays them out in real time.\n",
    "        \"\"\"\n",
    "        self.is_speaking = True\n",
    "        try:\n",
    "            while True:\n",
    "                response = await ws.recv()\n",
    "                data = json.loads(response)\n",
    "                \n",
    "                # Each 'response.audio.delta' event has a piece of audio data\n",
    "                if data[\"type\"] == \"response.audio.delta\":\n",
    "                    if \"delta\" in data:\n",
    "                        try:\n",
    "                            # Clean up the Base64 string\n",
    "                            audio_data = data[\"delta\"].replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "                            # Pad if the length isn't multiple of 4 (Base64 requirement)\n",
    "                            padding = len(audio_data) % 4\n",
    "                            if padding:\n",
    "                                audio_data += \"=\" * padding\n",
    "                            \n",
    "                            # Decode to raw PCM bytes, convert to np array, then play\n",
    "                            audio_bytes = base64.b64decode(audio_data)\n",
    "                            audio = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "                            self.output_stream.write(audio)\n",
    "                            \n",
    "                            # Print a dot to indicate streaming progress\n",
    "                            print(\".\", end=\"\", flush=True)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing audio: {e}\")\n",
    "                            \n",
    "                # 'response.done' means the assistant finished speaking this turn\n",
    "                elif data[\"type\"] == \"response.done\":\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            # Allow mic recording again\n",
    "            self.is_speaking = False\n",
    "\n",
    "async def main():\n",
    "    # Create our conversation system instance\n",
    "    system = ConversationSystem()\n",
    "    \n",
    "    # Set up mic and speaker streams\n",
    "    await system.setup_audio()\n",
    "    \n",
    "    # Start the real-time audio conversation\n",
    "    await system.start_conversation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting real-time conversation system...\")\n",
    "    # Run the main function in an asyncio event loop\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further iterations of the code, improve voice activity detection, handle interruptions, and manage context more effectively...\n",
    "part1_realtime_api_advanced_converse_step2_better_vad.py \n",
    "part1_realtime_api_advanced_converse_step3_interruption_handling.py\n",
    "part1_realtime_api_advanced_converse_step4_context_management.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 → Step 2: Adding Voice Activity Detection (VAD) and Basic Interrupt Handling\n",
    "\n",
    "Key Improvement: Introduced VAD for real-time speech detection instead of waiting for fixed intervals.\n",
    "New Code Snippet in Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=24000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.vad_threshold = 0.015  # Detects normal speech\n",
    "        self.speech_frames = 0\n",
    "        self.silence_frames = 0\n",
    "        self.min_speech_duration = int(0.3 * sample_rate)\n",
    "        self.max_silence_duration = int(0.8 * sample_rate)\n",
    "        self.main_buffer = []\n",
    "        self.speech_detected = False\n",
    "\n",
    "    def process_audio(self, indata):\n",
    "        \"\"\"Basic VAD: Only buffers speech detected above the threshold\"\"\"\n",
    "        audio_level = np.abs(indata).mean() / 32768.0  # Normalize audio levels\n",
    "\n",
    "        if audio_level > self.vad_threshold:\n",
    "            self.speech_detected = True\n",
    "            self.speech_frames += len(indata)\n",
    "            self.silence_frames = 0\n",
    "            self.main_buffer.extend(indata.tobytes())\n",
    "        elif self.speech_detected:\n",
    "            self.silence_frames += len(indata)\n",
    "            if self.silence_frames < self.max_silence_duration:\n",
    "                self.main_buffer.extend(indata.tobytes())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this an Improvement?\n",
    "\n",
    "✅ No more fixed 2-second waits (Step 1 was just waiting and then processing). \\\n",
    "✅ Real-time detection of speech (only records meaningful user input). \\\n",
    "✅ Ignores background noise, reducing unnecessary API calls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 → Step 3: Adding Barge-In Detection (Interruptions)\n",
    "\n",
    "Key Improvement: Detects if a user speaks while AI is responding, cancels AI output, and sends the interruption immediately.\n",
    "New Code Snippet in Step 3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=24000):\n",
    "        self.interrupt_threshold = 0.02  # Higher sensitivity for interruptions\n",
    "        self.interrupt_buffer = []\n",
    "        self.is_speaking = False\n",
    "        self.is_interrupting = False\n",
    "\n",
    "    def process_audio(self, indata):\n",
    "        \"\"\"Detects both normal speech and interruptions\"\"\"\n",
    "        audio_level = np.abs(indata).mean() / 32768.0\n",
    "\n",
    "        # If AI is speaking and user interrupts, switch to interruption buffer\n",
    "        if self.is_speaking and audio_level > self.interrupt_threshold:\n",
    "            self.is_interrupting = True\n",
    "            self.interrupt_buffer.extend(indata.tobytes())\n",
    "            return\n",
    "\n",
    "        # If AI is not speaking, process normally\n",
    "        if not self.is_speaking:\n",
    "            if audio_level > self.vad_threshold:\n",
    "                self.speech_detected = True\n",
    "                self.main_buffer.extend(indata.tobytes())\n",
    "            elif self.speech_detected:\n",
    "                self.silence_frames += len(indata)\n",
    "                if self.silence_frames < self.max_silence_duration:\n",
    "                    self.main_buffer.extend(indata.tobytes())\n",
    "\n",
    "    def check_interruption(self):\n",
    "        \"\"\"Returns True if the user interrupted\"\"\"\n",
    "        return self.is_interrupting\n",
    "\n",
    "    def get_interrupt_audio(self):\n",
    "        \"\"\"Retrieves buffered interruption speech\"\"\"\n",
    "        if not self.interrupt_buffer:\n",
    "            return None\n",
    "        audio_data = bytes(self.interrupt_buffer)\n",
    "        self.interrupt_buffer.clear()\n",
    "        self.is_interrupting = False\n",
    "        return audio_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this an Improvement? \\\n",
    "✅ Detects user interruptions dynamically. \\\n",
    "✅ Buffers user input while the AI is talking. \\\n",
    "✅ **Doesn’t wait for AI to finish—it immediately stops the AI and sends new input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 → Step 4: Cancelling AI Response in Real-Time\n",
    "\n",
    "Key Improvement: Now, if the user interrupts mid-response, the system sends a cancellation request (response.cancel) to Azure OpenAI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_response(self, websocket):\n",
    "    \"\"\"Handles AI responses and supports real-time interruption.\"\"\"\n",
    "    self.audio_processor.is_speaking = True\n",
    "    try:\n",
    "        while True:\n",
    "            # Check if user interrupted mid-response\n",
    "            if self.audio_processor.check_interruption():\n",
    "                interrupt_audio = self.audio_processor.get_interrupt_audio()\n",
    "                if interrupt_audio:\n",
    "                    print(\"Interrupted!\")\n",
    "                    \n",
    "                    # Cancel the AI’s current response\n",
    "                    await websocket.send(json.dumps({\"type\": \"response.cancel\"}))\n",
    "                    \n",
    "                    # Immediately process the new user input\n",
    "                    await self.send_audio(websocket, interrupt_audio)\n",
    "                    break\n",
    "            \n",
    "            response = json.loads(await websocket.recv())\n",
    "\n",
    "            if response[\"type\"] == \"response.audio.delta\":\n",
    "                if \"delta\" in response:\n",
    "                    try:\n",
    "                        audio_data = response[\"delta\"].strip()\n",
    "                        padding = -len(audio_data) % 4\n",
    "                        if padding:\n",
    "                            audio_data += \"=\" * padding\n",
    "                        \n",
    "                        audio = np.frombuffer(base64.b64decode(audio_data), dtype=np.int16)\n",
    "                        self.streams['output'].write(audio)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Audio processing error: {e}\")\n",
    "\n",
    "            elif response[\"type\"] == \"response.done\":\n",
    "                break\n",
    "    finally:\n",
    "        self.audio_processor.is_speaking = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this an Improvement? \\\n",
    "✅ No more waiting for AI to finish → Interruptions stop AI instantly. \\\n",
    "✅ Users don’t have to listen to an entire AI response before speaking. \\\n",
    "✅ Feels more like Alexa, Google Assistant, or Siri’s barge-in support. \n",
    "\n",
    "Difference between 2->3 and 3->4 is that step 3 adds real-time interruption detection while the AI is speaking, but it doesn’t yet cancel AI responses.\n",
    "These are included here for the sake of discussion as to the design choices that needs to be made while creating a Voice UI.\n",
    "Our intention here is not closing into perfection but to show an MVP that can be iterated upon...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                 | **Step 2** (VAD)       | **Step 3** (Barge-In Detection) | **Step 4** (AI Cancellation)  |\n",
    "|-------------------------|----------------------|--------------------------------|--------------------------------|\n",
    "| **Speech Detection**    | ✅ Uses VAD         | ✅ Uses VAD                   | ✅ Uses VAD                    |\n",
    "| **Interrupt Handling**  | ❌ No detection     | ✅ Detects interruptions but **AI keeps talking** | ✅ Detects interruptions and **AI stops talking** immediately |\n",
    "| **AI Response Handling** | ❌ Always waits for AI to finish | ❌ Always waits for AI to finish | ✅ **Cancels AI immediately** when interrupted |\n",
    "| **Natural Conversation** | ❌ Feels robotic (fixed waits) | ❌ User can talk over AI, but AI doesn’t stop | ✅ **User can talk over AI, and AI stops instantly** |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
